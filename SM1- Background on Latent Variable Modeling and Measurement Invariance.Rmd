---
title: " "
author: "Meltem Ozcan"
bibliography: bibliography.bib
output: pdf_document
header-includes: \usepackage{setspace}\doublespacing
---

\doublespacing 

# Background

## Latent Variable Modeling (LVM)

At the core of latent variable modeling is the idea that unobserved latent constructs influence observed variables [@maydeu2009sage]. For instance, an individual's underlying math ability can be expected to affect their performance on a math test. The common factor model [@thurstone1947] posits that the relationship between latent constructs (e.g., math skill) and observed variables (e.g., scores on a math test) can be expressed using a system of equations. We can represent the relationship between $J$ items ($j=1,\ldots,J$) and $M$ latent constructs ($m=1,\ldots,M$) in matrix format as the following multiple-group factor analysis model
$$
\boldsymbol{y}_{ig} = \boldsymbol{\nu}_g + \boldsymbol{\Lambda}_g \boldsymbol{\eta}_{ig} +\boldsymbol{\epsilon}_{ig}
$$
where $\boldsymbol{y}_{ig}$ is a $J \times 1$ vector of observed item scores for individual $i$ from group $g$, $\boldsymbol{\eta}_{ig}$ indicates a $M \times 1$ vector of latent factor scores, $\boldsymbol{\nu}_g$ is a $J \times 1$ vector of intercepts, $\boldsymbol{\Lambda}_{g}$ is a $J \times M$ matrix of factor loadings, and $\boldsymbol{\epsilon}_{ig}$ is a $J \times 1$ vector of unique factor variables. The latent factor scores are assumed to be distributed with $M \times 1$ mean vector E$(\boldsymbol{\eta})=\boldsymbol{\alpha}$ and $M \times M$ variance-covariance matrix Cov$(\boldsymbol{\eta})=\boldsymbol{\Psi}$. The unique factor variables are distributed with mean E$(\boldsymbol{\epsilon})=0$ and variance-covariance matrix Cov$(\boldsymbol{\epsilon})=\Theta$. This relationship can also be expressed as the following system of equations at the item level:

$$
y_{ijg} = {\nu}_{jg} + {\lambda}_{jg} {\eta}_{ijg} + {\epsilon}_{ijg}
$$ 
where $\epsilon_{ijg}$ and $\eta_{ijg}$ are independent.

As latent variables lack an inherent or universal scale, such a system suffers from indeterminacy, meaning that there exists no unique solution until some identification constraints are set to provide a scale to the latent variables [@bollen2014structural; @maydeu2009sage]. For instance, the system can be identified by setting the latent mean and variances to 0 and 1 respectively, allowing for the comparison of factor loadings and intercepts across groups without an anchor variable [@van2012checklist]. In this approach, the latent variable is standardized and all loadings are estimated freely. Alternatively, a reference indicator may be chosen and its loading and intercept may be constrained to 1 and 0 respectively. This approach sets the scale of the latent variable to the scale of the observed reference variable. Remaining loadings are estimated relative to the reference indicator, and the latent variance is freely estimated.

Once identified, the system of equations can be used to obtain composite sum or average scores or factor scores, which can be used in subsequent analyses investigating group-level differences or as predictor variables in prediction problems [@maydeu2009sage]. Composite sum or average scores may be obtained by simply summing or averaging the raw scores on each item. This simplicity is made possible through several stringent assumptions that are unlikely to hold in practice, such as the unidimensionality of the construct, no measurement error, equal reliability across all items, and a perfectly linear relationship between each item and the latent construct [@maydeu2009sage; @distefano2019understanding; @grice2001computing]. As equal weights are assigned to each item, the sum/average scoring approach assumes that each item contributes equally to the underlying construct. However, this assumption can result in biased or inaccurate scores, particularly when certain items have a stronger relationship with the latent factor than others or when some items are less reliable, thus failing to account for differences in item quality and relevance. 

Alternative, more refined [@grice2001computing] approaches have been developed to address these limitations for continuous data, such as factor score estimation methods like Bartlett scores [@bartlett1937statistical], which accounts for the varying contributions of each item by weighting them according to their relationship with the latent construct, providing more accurate and reliable estimates of the underlying factor. Bartlett scores are designed to provide unbiased estimates of the true factor scores that are highly correlated with the common factors by minimizing the error variance (the unique factors) via a least squares approach [@grice2001computing; @distefano2019understanding]. Bartlett factor scores are estimated using the following formula:
$$
\widehat{\boldsymbol{F}} =  (\boldsymbol{\Lambda}'\boldsymbol{\Theta}^{-1}\boldsymbol{\Lambda})^{-1}\boldsymbol{\Lambda}'\boldsymbol{\Theta}^{-1}\boldsymbol{y}
$$
where $\boldsymbol{y}$ is the vector of observed item scores, $\boldsymbol{\Theta}^{-1}$ contains the inverse diagonal of the unique factor variances, and $\boldsymbol{\Lambda}$ is the factor pattern matrix of loadings [@lai2023correcting].

## Measurement Invariance (MI)

Observed test scores can only be considered comparable when measurements are on the same scale and latent construct(s) $\eta$ are measured equivalently and comparably across groups and/or conditions (g) such that $P(y|\eta, W = g) = P(y|\eta)$, $\forall g$, i.e., when measurement invariance (MI) holds [@mellenbergh1989item; @widaman1997exploring]. In other words, MI holds when the relationships between test items and the latent construct(s) are identical between groups and/or conditions. Systematic differences in measurement operations across grouping variables indicate measurement noninvariance, which may result in spurious inferences and make it impossible to separate the observed effects from construct-irrelevant attributes [@meredith1993measurement; @widaman1997exploring]. 

In a factor analytic framework, the parameters of a system relating test items to a latent variable can be estimated and tested for equivalence using confirmatory factor analysis [@Joreskog1969]. The equivalence or nonequivalence of sets of item parameters determine the level of invariance, which are nested hierarchically [@meredith2006essay; @meredith1993measurement]. Configural invariance [@horn1992practical] is said to hold when the factor structure (i.e., the configuration of the items and the factors) is equivalent across groups such that the pattern of zero and nonzero factor loadings is the same across groups. If configural invariance holds, the test measures similar (but not necessarily the same) latent constructs across groups [@widaman1997exploring]. The factor means and variances are not identified in the configural model, and as different methods of identifying the model can lead to different parameter estimates under the configural model, group differences cannot be tested at this level [@widaman1997exploring]. Metric (also called pattern or weak) invariance [@horn1992practical; @meredith2006essay] holds if loadings are equivalent across groups, suggesting equivalence of the strength and direction of the linear relationship between items and latent constructs across groups. With metric invariance, scores from different groups can be said to have the same unit or interval. Scalar (also called strong) invariance holds if in addition to loadings, intercepts are equivalent across groups. Scalar invariance is necessary for factor means to be comparable, and suggests that the meanings of the items as well as the levels of item responses are invariant across groups [@millsap2011; @meredith2006essay; @widaman1997exploring; @van2015measurement]. Strict invariance holds if all measurement parameters (i.e., loadings, intercepts, and uniqueness) are equivalent across the grouping variables, making any group-level differences in the observed variables entirely attributable to group-level differences in the underlying construct as it is measured identically across groups [@widaman1997exploring; @van2012checklist]. The complete equivalence of measurement parameters necessitated by the strict invariance model is difficult to attain in practice [@van2015measurement], and most often, a partial invariance model [@byrne1989testing] with a subset of invariant items is determined to enable group-level comparisons. A minimum of two items with scalar invariance (i.e., equal loadings and intercepts across groups) is needed for the comparison of latent factor means [@byrne1989testing].

<!-- a slightly shorter version of the paragraph below is in the main manuscript -->
Testing begins with an unconstrained configural model reflecting the theoretical operationalization of the construct. If the configural model does not have acceptable fit, items may not be measuring the same construct across groups and testing may not proceed. If configural invariance is established, increasingly strict equality constraints are placed on sets of parameters (loadings, intercepts, and uniqueness) across groups to test for metric, scalar, and strict invariance (i.e., from the bottom of the hierarchy of measurement invariance stages to the top) in a sequential (stepwise) specification search where the lower stages need to be cleared at least partially to test for higher levels of invariance. Model fit may be compared with tests such as the likelihood ratio (LRT) $\chi^2$ test [@cochran1952chi2]. If a particular stage of invariance does not hold (e.g., indicated by a significant $\chi^2$ test), parameters are relaxed one at a time to test for partial invariance at that stage. Fit of the more and less constrained models are compared sequentially, and the parameter leading to the largest $\chi^2$ difference is released. This process of model refitting and comparison continues from the lowest stage of invariance (metric) to the highest (strict) until the LRT is nonsignificant or there are no additional parameters that may be released, arriving at a partial invariance model <!--manual adjustments are made to the models by releasing or adding constraints to arrive at a partial invariance model--> [@van2012checklist]. Note if any loadings are released in the partial metric model, the intercepts of noninvariant items are freed in the following models in line with established guidelines [@putnick2016measurement]. Modification indices [@sorbom1989model; @yoon2007detecting; @schmitt2008measurement; @yoon2014comparison] may also be examined to assess the improvement in the model fit if a parameter was freely estimated vs. constrained. 

While MI testing is commonly performed in the context of grouping variables such as ethnicity, gender, or time, it may also be of interest to test the equivalence of measurement operations across different studies. When pooling data from multiple studies, establishing MI within each study does not necessarily translate to achieving MI across the studies, or that the measure(s) administered in the studies are comparable [@curranhussong2009]. As such, MI testing may be performed considering study membership as a grouping variable.

\newpage

## References
